{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce8ead91-4cf9-4dab-be91-3d3896d21889",
   "metadata": {},
   "source": [
    "# Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80469184-0ebd-40b2-9688-b9824172730e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'<[^>]*?>', '', text)\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9 ]', '', text)\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "    text = text.strip()\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b407cc18-3240-40fb-b596-368fadb04bc6",
   "metadata": {},
   "source": [
    "# JD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ee09a9e0-1d02-4b81-a1ef-7276bc4ae348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'Data Engineer',\n",
       " 'experience': '2 years of experience in the job offered or in a data-related occupation',\n",
       " 'skills': ['System development life cycle',\n",
       "  'Cloud platforms such as AWS and Databricks',\n",
       "  'SQL',\n",
       "  'Version control and CI/CD pipelines',\n",
       "  'Extracting, transforming, and loading and data pipelines',\n",
       "  'Scripting and automation',\n",
       "  'Big data technologies such as Hadoop and Spark',\n",
       "  'Data Warehouse concepts and methodologies',\n",
       "  'Relational and non-relational database design',\n",
       "  'Programming languages such as Python, Java, and Scala'],\n",
       " 'description': 'Design and build simple, reusable components of larger process or framework to support analytics products with mentorship from experienced peers, design and implement product features in collaboration with Business and Technology partners, clean, prepare, and optimize data at scale for ingestion and consumption, support the implementation of new data management projects and restructure of the current data architecture, implement automated workflows and routines using workflow scheduling tools, understand and use continuous integration, test-driven development, and production deployment frameworks, anticipate, identify, and solve issues concerning data management to improve data quality, participate in design, code, test plans, and dataset implementation performed by other data engineers in support of maintaining data engineering standards, analyze and profile data for the purpose of designing scalable solutions and solve straightforward data issues and perform root cause analysis to proactively resolve product issues'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.output_parsers.json import JsonOutputParser\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "# from utils.clean import clean_text\n",
    "from templates.extractPattrens import jobDiscritionFormat\n",
    "\n",
    "\n",
    "class JobDescriptionExtractor:\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        self.jsonParser = JsonOutputParser()\n",
    "\n",
    "    def scrapeWebsite(self, url):\n",
    "        loader = WebBaseLoader(web_path=url)\n",
    "        response = loader.load()\n",
    "        cleaned_response = clean_text(response[0].page_content)\n",
    "        return cleaned_response\n",
    "\n",
    "    def promptTemplate(self):\n",
    "        return PromptTemplate.from_template(\n",
    "            \"\"\"\n",
    "            ### SCRAPED TEXT FROM WEBSITE:\n",
    "            {page_data}\n",
    "            ### INSTRUCTION:\n",
    "            The scraped text is from the career's page of a website.\n",
    "            Your job is to extract the job postings and return them in following pattern {jobDiscritionFormat}\n",
    "            Only return the valid JSON.\n",
    "            ### VALID JSON (NO PREAMBLE):\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "    def extractJD(self, url: str):\n",
    "        cleaned_response = self.scrapeWebsite(url)\n",
    "        prompt_template = self.promptTemplate()\n",
    "        json_parser = JsonOutputParser()\n",
    "\n",
    "        chain = prompt_template | self.llm \n",
    "        jd_response = chain.invoke({\n",
    "            \"page_data\": cleaned_response,\n",
    "            \"jobDiscritionFormat\": jobDiscritionFormat\n",
    "        })\n",
    "        jd_response = self.jsonParser.parse(jd_response.content)\n",
    "        return jd_response\n",
    "\n",
    "\n",
    "# -----------------------> testing <-------------------------\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "load_dotenv()\n",
    "GROQ_API = os.getenv(\"GROQ\")\n",
    "llm = ChatGroq(temperature=0, groq_api_key=GROQ_API, model=\"llama-3.3-70b-versatile\")\n",
    "jd = JobDescriptionExtractor(llm)\n",
    "jdRes = jd.extractJD(\"https://careers.nike.com/data-engineer/job/R-61137\")\n",
    "jdRes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d243549a-087d-4af2-a3f4-0f7d6d1741e5",
   "metadata": {},
   "source": [
    "# CreateChroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e0630093-2087-42ed-a00f-55c959ba3591",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import uuid\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "class ProjectVectorStore:\n",
    "    def __init__(self, name, projects, jdSkills, numProjects = 3,storage_path=\"./chroma_storage\"):\n",
    "        self.projects = projects\n",
    "        self.jdSkills = jdSkills\n",
    "        self.numProjects = numProjects\n",
    "        self.user_name = self._format_name(name)\n",
    "        self.client = chromadb.PersistentClient(path=storage_path)\n",
    "        self.collection = self.client.get_or_create_collection(name=f\"projects_{self.user_name}\")\n",
    "\n",
    "    def _format_name(self, name: str) -> str:\n",
    "        return name.lower().replace(\" \", \"_\")\n",
    "    \n",
    "    def addDocuments(self):\n",
    "        for project in self.projects:\n",
    "            self.collection.add(\n",
    "                documents = list(project[\"TechstackUsed\"]),  \n",
    "                metadatas = [{\n",
    "                    \"ProjectTitle\": project[\"ProjectTitle\"],\n",
    "                    \"description\": project[\"description\"],\n",
    "                    \"GitHub_Link\": project[\"GitHub_Link\"]\n",
    "                    }] * len(project[\"TechstackUsed\"]),\n",
    "                    ids = [str(uuid.uuid4()) for _ in project[\"TechstackUsed\"]])\n",
    "\n",
    "    def _query(self):\n",
    "        projectResponse = self.collection.query(query_texts = self.jdSkills)\n",
    "        return projectResponse\n",
    "\n",
    "    def retriveProjects(self):\n",
    "        self.addDocuments()\n",
    "        projectResponse = self._query()\n",
    "        allProjects = list(chain.from_iterable(projectResponse[\"metadatas\"]))\n",
    "        seenProjectTitles = set()\n",
    "        uniqueProjects = []\n",
    "        for project in allProjects:\n",
    "            title = project.get(\"ProjectTitle\", \"\").strip()\n",
    "            if title and title not in seenProjectTitles:\n",
    "                seenProjectTitles.add(title)\n",
    "                uniqueProjects.append({\n",
    "                    \"ProjectTitle\": title,\n",
    "                    \"description\": project.get(\"description\", \"\").strip(),\n",
    "                    \"GitHub_Link\": project.get(\"GitHub_Link\", \"\").strip()\n",
    "                })\n",
    "            if len(uniqueProjects) == self.numProjects:\n",
    "                break\n",
    "\n",
    "        return uniqueProjects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec7f102-eebd-4012-ae13-4388a77c2624",
   "metadata": {},
   "source": [
    "# UserInformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc64e4a1-707a-4526-a77f-3e45811cc020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BasicInfo': {'name': 'Gandluru Mohammed Yaseen',\n",
       "  'linkedinLink': 'None',\n",
       "  'github': 'https://github.com/yaseeng-md',\n",
       "  'mail': 'gandlurumohammedyaseen@gmail.com',\n",
       "  'mobile': '8328377285'},\n",
       " 'Skills': {'ProgrammingLanguages': ['Python'],\n",
       "  'FrameWorks': ['PyTorch', 'TensorFlow'],\n",
       "  'Tools/Platform': ['Serper API',\n",
       "   'BeautifulSoup',\n",
       "   'Hugging Face models',\n",
       "   'Linux'],\n",
       "  'SoftSkills': ['None']},\n",
       " 'Experience': [{'Designation': 'Intern',\n",
       "   'Role': 'Data Valley',\n",
       "   'Work': 'Implemented a Recommendation System for Movie Recommendations, Proposed solutions for problems using Machine Learning and Artificial Intelligence',\n",
       "   'TechStack': ['Machine Learning', 'Artificial Intelligence'],\n",
       "   'Duration': 'January 2024 â€“ April 2024'}],\n",
       " 'Projects': [{'ProjectTitle': 'Fine Tuning Llama Model for Website Article Summarization',\n",
       "   'GitHub_Link': 'None',\n",
       "   'description': 'Fine-tuning a LLaMA 2 7B model for automatic summarization of website articles, Utilizing the Serper API to retrieve web pages, and BeautifulSoup for parsing and extracting article content, Achieve a better ROUGE score',\n",
       "   'TechstackUsed': ['LLaMA 2 7B', 'Serper API', 'BeautifulSoup']},\n",
       "  {'ProjectTitle': 'SAR Image Colorization Using Deep Learning Model',\n",
       "   'GitHub_Link': 'https://github.com/yaseeng-md/SAR_Image_Colorization_Using_Deep_Learning_Algorithms',\n",
       "   'description': 'Engineered advanced GAN architectures for SAR image colorization and enhancement, Used Pix2Pix GAN with Perceptual Loss, Achieved 0.97 SSIM, 27.42 dB PSNR, and 0.0021 MSE, A jump of 510% in SSIM and 143% in PSNR compared to the baseline',\n",
       "   'TechstackUsed': ['GAN', 'Pix2Pix GAN', 'Perceptual Loss']},\n",
       "  {'ProjectTitle': 'Explicit Content Detection using Attention Mechanisms',\n",
       "   'GitHub_Link': 'https://github.com/yaseeng-md/Explicit-Content-Detection',\n",
       "   'description': 'Developed cutting-edge deep learning models using Vision Transformer (ViT) and Swin ViT for high-precision feature extraction, Achieved a remarkable 98.35% accuracy by combining Convolution-based Patch Extraction with Swin Vision Transformer',\n",
       "   'TechstackUsed': ['Vision Transformer (ViT)',\n",
       "    'Swin ViT',\n",
       "    'PyTorch',\n",
       "    'Hugging Face models',\n",
       "    'Linux']},\n",
       "  {'ProjectTitle': 'Audio Deepfake Detection',\n",
       "   'GitHub_Link': 'https://github.com/yaseeng-md/Audio-DeepFake-Detection',\n",
       "   'description': 'Proposed an advanced audio deepfake detection system using CNN, RNN, and LSTM architecture, Leveraged Mel-Frequency Cepstral Coefficients (MFCC) and Linear Frequency Cepstral Coefficients (LFCC) to improve model accuracy, Achieved 98% with MFCC-LSTM and 96% with LFCC-LSTM',\n",
       "   'TechstackUsed': ['CNN', 'RNN', 'LSTM', 'MFCC', 'LFCC']}]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from templates.extractPattrens import resumeExtractFormat\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers.json import JsonOutputParser\n",
    "\n",
    "class UserInformationExtractor:\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        self.prompt_template = PromptTemplate.from_template(\n",
    "            \"\"\"\n",
    "            The Resume information is {page_content}\n",
    "            You are a smart assistant, and I have provided you a text.\n",
    "            Your task is to return me details in format of {resumeExtractFormat} and for discription include the summary of discription of that project with numerical results avilable.\n",
    "            Return strictly in JSON format, and if anything is unknown placeholder is \"None\".\n",
    "            If there are many projects, append them in a list of JSON. \n",
    "            NO PREAMBLE\n",
    "            \"\"\"\n",
    "        )\n",
    "        self.parser = JsonOutputParser()\n",
    "\n",
    "    def extract_from_pdf(self, path: str) -> dict:\n",
    "        loader = PyPDFLoader(path)\n",
    "        pages = loader.load()\n",
    "        page_content = pages[0].page_content\n",
    "        return self._extract(page_content)\n",
    "\n",
    "    def extract_from_text(self, text: str) -> dict:\n",
    "        return self._extract(text)\n",
    "\n",
    "    def _extract(self, page_content: str) -> dict:\n",
    "        chain = self.prompt_template | self.llm\n",
    "        response = chain.invoke({\n",
    "            \"page_content\": page_content,\n",
    "            \"resumeExtractFormat\": resumeExtractFormat\n",
    "        })\n",
    "        return self.parser.parse(response.content)\n",
    "\n",
    "\n",
    "candidateInformationExtractrorPy = UserInformationExtractor(llm)\n",
    "candidateInformation = candidateInformationExtractrorPy.extract_from_pdf(\"E:/Resumes/Specalized Resume.pdf\") \n",
    "candidateInformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5ab01c05-e7aa-4d17-ba18-cf0f0a7bc0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from utils.createChroma import ProjectVectorStore\n",
    "\n",
    "import streamlit as st\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "class Pipeline:\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "\n",
    "    # -------------------> Extraction Methods <-------------------\n",
    "\n",
    "    def extract_candidate_information(self, isPDF: bool, page_content: str = None, path: str = None):\n",
    "        extractor = UserInformationExtractor(self.llm)\n",
    "\n",
    "        if isPDF:\n",
    "            if not path:\n",
    "                raise ValueError(\"PDF path is missing.\")\n",
    "            candidateInformation =  extractor.extract_from_pdf(path)  # Sync version\n",
    "        else:\n",
    "            if not page_content:\n",
    "                raise ValueError(\"Resume text is missing.\")\n",
    "            candidateInformation =  extractor.extract_from_text(page_content)\n",
    "        # print(candidateInformation)\n",
    "        return candidateInformation\n",
    "\n",
    "\n",
    "    def extract_job_description(self, url: str):\n",
    "        jd = JobDescriptionExtractor(self.llm)\n",
    "        jdJson = jd.extractJD(url)\n",
    "        # print(jdJson)\n",
    "        return jdJson\n",
    "\n",
    "    # -------------------> Chroma Vector DB <-------------------\n",
    "\n",
    "    def createOrGetVDB(self, name, projects, jdSkills, numProjects = 3):\n",
    "        chroma = ProjectVectorStore(name, projects, jdSkills,numProjects)\n",
    "        uniqueProjects = chroma.retriveProjects()\n",
    "        # print(uniqueProjects)\n",
    "        return uniqueProjects\n",
    "\n",
    "    # -------------------> Email Generation <-------------------\n",
    "\n",
    "    def generateEmailPrompt(self):\n",
    "        prompt = \"\"\"\n",
    "        ## Job Discription: {jdJson}\n",
    "        ## Candidate Information\n",
    "        Your name is {name} and your job is to write an cold email asking for Job asking. You have a previous experince as {experinece}.\n",
    "        These are basic details {BasicInfo}, your projects are {projects}, and skills is {skills}.\n",
    "        Express your projects at same place, and include github links if any.\n",
    "        Include your qualifications, so that you can get a job. Maintian formal speech tone across mail. And add links to contact like LinkedinLink, GitHub and personal website if any. \n",
    "        ## NO PREAMBLE\n",
    "        \"\"\"\n",
    "\n",
    "        return PromptTemplate.from_template(prompt)\n",
    "\n",
    "    def generateEmail(self,jdJson,jsonCandidate,projects):\n",
    "        mail_prompt = self.generateEmailPrompt()\n",
    "        mailLLM = mail_prompt | self.llm\n",
    "        mailRes = mailLLM.invoke(input={\"jdJson\":jdJson, \"name\" : jsonCandidate[\"BasicInfo\"][\"name\"] ,\"experinece\" : jsonCandidate[\"Experience\"], \n",
    "                     \"BasicInfo\" : jsonCandidate[\"BasicInfo\"], \"projects\" : projects, \"skills\" : jsonCandidate[\"Skills\"] }) \n",
    "        return mailRes.content\n",
    "\n",
    "\n",
    "# load_dotenv()\n",
    "# GROQ_API = os.getenv(\"GROQ\")\n",
    "# llm = ChatGroq(temperature=0, groq_api_key=GROQ_API, model=\"llama-3.3-70b-versatile\")\n",
    "# pipeline = Pipeline(llm)\n",
    "# candidateInformation = pipeline.extract_candidate_information(isPDF=True,path = r\"E:/Resumes/Specalized Resume.pdf\")\n",
    "# jdJson = pipeline.extract_job_description(\"https://careers.nike.com/data-engineer/job/R-61137\")\n",
    "# uniqueProjects = pipeline.createOrGetVDB(candidateInformation[\"BasicInfo\"][\"name\"], candidateInformation[\"Projects\"],jdJson[\"skills\"])\n",
    "# mailRes = pipeline.generateEmail(jdJson,candidateInformation,uniqueProjects)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
